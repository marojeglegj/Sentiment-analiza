{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b022260e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,roc_auc_score\n",
    "import classla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31691c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data: 10388\n"
     ]
    }
   ],
   "source": [
    "train_df=pd.read_csv(\"train.tsv\",sep='\\t')\n",
    "test_df=pd.read_csv(\"test.tsv\",sep='\\t')\n",
    "eval_df=pd.read_csv(\"eval.tsv\",sep='\\t')\n",
    "\n",
    "sve=pd.concat([train_df,test_df,eval_df])\n",
    "print(f\"Total data: {len(sve)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "080ab36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary data: 3321\n",
      "Pozitivni: 2031\n",
      "Negativni: 1290\n"
     ]
    }
   ],
   "source": [
    "binary_data=sve[sve['label'].isin([0,2])].copy()\n",
    "binary_data['sentiment']=binary_data['label'].map({0:0,2:1})\n",
    "print(f\"Binary data: {len(binary_data)}\")\n",
    "print(f\"Pozitivni: {len(binary_data[binary_data['sentiment']==1])}\")\n",
    "print(f\"Negativni: {len(binary_data[binary_data['sentiment']==0])}\")\n",
    "\n",
    "binary_data.to_csv(\"Binary data\", index=False, encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1f84810",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=classla.Pipeline('hr',processors='tokenize,pos,lemma', verbose=False,use_gpu=False)\n",
    "\n",
    "stop_words = {'i', 'je', 'u', 'na', 'se', 'da', 'su', 'za', 'sa', 's', 'od', 'do', 'po', 'iz', 'o', \n",
    "              'a', 'ali', 'ili', 'pa', 'te', 'kad', 'kao', 'što', 'gdje', 'koji', \n",
    "              'koja', 'koje', 'bi', 'će', 'ti', 'mu', 'ga', 'ju', 'me', 'mi', 'si', 'to', 'tu', \n",
    "              'ta', 'ova', 'ovaj', 'sve', 'svoj', 'ima', 'sam', 'su', 'bio', 'bila', 'bilo'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23c6b24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_content_words_only(text):\n",
    "    text = str(text).lower()\n",
    "    doc = nlp(text)\n",
    "    words = []\n",
    "    \n",
    "    # Keep only nouns, verbs, adjectives, and adverbs\n",
    "    content_pos = {'NOUN', 'VERB', 'ADJ', 'ADV','AUX'}\n",
    "    \n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            if word.upos in content_pos:  # upos = universal part-of-speech tag\n",
    "                lemma = word.lemma.lower()\n",
    "                words.append(lemma)\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f45f93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text_content_words_only_neg(text):\n",
    "    text = str(text).lower()\n",
    "    text=re.sub(r'[^a-zA-ZčćđšžČĆĐŠŽ\\s]','',text)\n",
    "    text=' '.join(text.split())\n",
    "    \n",
    "    # Handle negation before lemmatization - using word boundaries\n",
    "    text = re.sub(r'\\bnije\\b', 'NEG_biti', text)\n",
    "    text = re.sub(r'\\bne\\b', 'NEG_biti', text)\n",
    "    text = re.sub(r'\\bneću\\b', 'NEG_htjeti', text)\n",
    "    text = re.sub(r'\\bnetreba\\b', 'NEG_trebati', text)\n",
    "    text = re.sub(r'\\bneće\\b', 'NEG_htjeti', text)\n",
    "    text = re.sub(r'\\bnisam\\b', 'NEG_biti', text)\n",
    "    text = re.sub(r'\\bnisi\\b', 'NEG_biti', text)\n",
    "    text = re.sub(r'\\bnismo\\b', 'NEG_biti', text)\n",
    "    text = re.sub(r'\\bniste\\b', 'NEG_biti', text)\n",
    "    text = re.sub(r'\\bnisu\\b', 'NEG_biti', text)\n",
    "    text = re.sub(r'\\bnećeš\\b', 'NEG_htjeti', text)\n",
    "    text = re.sub(r'\\bnećete\\b', 'NEG_htjeti', text)\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    words = []\n",
    "    \n",
    "    # Keep only nouns, verbs, adjectives, and adverbs\n",
    "    content_pos = {'NOUN', 'VERB', 'ADJ', 'ADV','AUX'}\n",
    "    \n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            # Handle NEG_ words manually since CLASSLA won't recognize them\n",
    "            if word.text.startswith('NEG_'):\n",
    "                words.append(word.text.lower())\n",
    "            elif word.upos in content_pos:\n",
    "                lemma = word.lemma.lower()\n",
    "                words.append(lemma)\n",
    "    \n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "409f35b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_slova(text):\n",
    "    text=str(text).lower()\n",
    "    text=re.sub(r'[^a-zA-ZčćđšžČĆĐŠŽ\\s]','',text)\n",
    "    text=' '.join(text.split())\n",
    "    doc=nlp(text)\n",
    "    words=[]\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            lemma=word.lemma.lower()\n",
    "            if len(lemma)>2 and lemma not in stop_words:\n",
    "                words.append(lemma)\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21c1a389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_slova_neg(text):\n",
    "    text=str(text).lower()\n",
    "    text=re.sub(r'[^a-zA-ZčćđšžČĆĐŠŽ\\s]','',text)\n",
    "    text=' '.join(text.split())\n",
    "\n",
    "    text = re.sub(r'\\bnije\\b', 'NEG_biti', text)\n",
    "    text = re.sub(r'\\bne\\b', 'NEG_biti', text)\n",
    "    text = re.sub(r'\\bneću\\b', 'NEG_htjeti', text)\n",
    "    text = re.sub(r'\\bnetreba\\b', 'NEG_trebati', text)\n",
    "    text = re.sub(r'\\bneće\\b', 'NEG_htjeti', text)\n",
    "    text = re.sub(r'\\bnisam\\b', 'NEG_biti', text)\n",
    "    text = re.sub(r'\\bnisi\\b', 'NEG_biti', text)\n",
    "    text = re.sub(r'\\bnismo\\b', 'NEG_biti', text)\n",
    "    text = re.sub(r'\\bniste\\b', 'NEG_biti', text)\n",
    "    text = re.sub(r'\\bnisu\\b', 'NEG_biti', text)\n",
    "    text = re.sub(r'\\bnećeš\\b', 'NEG_htjeti', text)\n",
    "    text = re.sub(r'\\bnećete\\b', 'NEG_htjeti', text)\n",
    "\n",
    "    doc=nlp(text)\n",
    "    words=[]\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            if word.text.startswith('NEG_'):\n",
    "                words.append(word.text.lower())\n",
    "            else: \n",
    "                lemma=word.lemma.lower()\n",
    "                if (len(lemma)>2 and lemma not in stop_words):\n",
    "                    words.append(lemma)\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3ffa028",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_data_pos=binary_data.copy()\n",
    "binary_data_pos_neg=binary_data.copy()\n",
    "binary_data_slova=binary_data.copy()\n",
    "binary_data_slova_neg=binary_data.copy()\n",
    "binary_data_slova['processed']=binary_data_slova['text'].apply(clean_text_slova)\n",
    "binary_data_slova_neg['processed']=binary_data_slova_neg['text'].apply(clean_text_slova_neg)\n",
    "binary_data_pos['processed']=binary_data_pos['text'].apply(clean_text_content_words_only)\n",
    "binary_data_pos_neg['processed']=binary_data_pos_neg['text'].apply(clean_text_content_words_only_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d04e1778",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balansiraj_dataset(data):\n",
    "    negativni=data[data['sentiment']==0]\n",
    "    pozitivni=data[data['sentiment']==1]\n",
    "    n_samples=min(len(negativni),len(pozitivni))\n",
    "    balanced=pd.concat([negativni.sample(n=n_samples,random_state=42),pozitivni.sample(n=n_samples,random_state=42)])\n",
    "    return balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e37ac1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SlovaNeg: 2580\n",
      "Slova: 2580\n",
      "POSNEG: 2580\n",
      "POS: 2580\n"
     ]
    }
   ],
   "source": [
    "balanced_slova_neg=balansiraj_dataset(binary_data_slova_neg)\n",
    "balanced_slova=balansiraj_dataset(binary_data_slova)\n",
    "balanced_pos=balansiraj_dataset(binary_data_pos)\n",
    "balanced_data_pos_neg=balansiraj_dataset(binary_data_pos_neg)\n",
    "print(f\"SlovaNeg: {len(balanced_slova_neg)}\")\n",
    "print(f\"Slova: {len(balanced_slova)}\")\n",
    "print(f\"POSNEG: {len(balanced_data_pos_neg)}\")\n",
    "print(f\"POS: {len(balanced_pos)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47634b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "def test_naive_bayes_combined(data, dataset_name):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Testiranje: {dataset_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    X = data['processed']\n",
    "    y = data['sentiment']\n",
    "\n",
    "    # Train/Test Split Analysis\n",
    "    print(\"TRAIN/TEST SPLIT RESULTS:\")\n",
    "    print(\"-\" * 30)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    print(f\"Train uzoraka: {len(X_train)}\")\n",
    "    print(f\"Test uzoraka: {len(X_test)}\")\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,1))\n",
    "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "    X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "    model = MultinomialNB(alpha=0.5)\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test_tfidf)\n",
    "    y_proba = model.predict_proba(X_test_tfidf)[:,1]\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy: .3f} ({accuracy:.1%})\")\n",
    "    print(f\"AUC: {auc:.3f}\")\n",
    "    print(f\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    # Save misclassified examples to CSV files\n",
    "    X_test_original = data.loc[X_test.index, 'text']  # Get original texts\n",
    "    \n",
    "    misclassified_data = []\n",
    "    for i in range(len(y_test)):\n",
    "        if y_test.iloc[i] != y_pred[i]:\n",
    "            text = X_test_original.iloc[i]\n",
    "            actual = y_test.iloc[i]\n",
    "            predicted = y_pred[i]\n",
    "            text_length = len(text)\n",
    "            \n",
    "            misclassified_data.append({\n",
    "                'text': text,\n",
    "                'actual_label': actual,\n",
    "                'predicted_label': predicted,\n",
    "                'text_length': text_length,\n",
    "                'error_type': 'False Positive' if (actual == 0 and predicted == 1) else 'False Negative'\n",
    "            })\n",
    "    \n",
    "    # Convert to DataFrame and sort by length (shortest first)\n",
    "    misclassified_df = pd.DataFrame(misclassified_data)\n",
    "    misclassified_df = misclassified_df.sort_values('text_length')\n",
    "    \n",
    "    # Save CSV files\n",
    "    filename_base = dataset_name.replace(' ', '_').lower()\n",
    "    misclassified_df.to_csv(f'{filename_base}_misclassified.csv', index=False, encoding='utf-8')\n",
    "    \n",
    "    false_positives = misclassified_df[misclassified_df['error_type'] == 'False Positive'].sort_values('text_length')\n",
    "    false_negatives = misclassified_df[misclassified_df['error_type'] == 'False Negative'].sort_values('text_length')\n",
    "    \n",
    "    false_positives.to_csv(f'{filename_base}_false_positives.csv', index=False, encoding='utf-8')\n",
    "    false_negatives.to_csv(f'{filename_base}_false_negatives.csv', index=False, encoding='utf-8')\n",
    "    \n",
    "    print(f\"\\nMisclassified: {len(misclassified_df)} total\")\n",
    "    print(f\"False Positives: {len(false_positives)}\")\n",
    "    print(f\"False Negatives: {len(false_negatives)}\")\n",
    "    print(f\"CSV files saved: {filename_base}_misclassified.csv, {filename_base}_false_positives.csv, {filename_base}_false_negatives.csv\")\n",
    "    \n",
    "    # Cross-Validation Analysis\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"CROSS-VALIDATION RESULTS:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # TF-IDF vektorizacija na cijelom datasetu\n",
    "    vectorizer_cv = TfidfVectorizer(max_features=5000, ngram_range=(1,1))\n",
    "    X_tfidf = vectorizer_cv.fit_transform(X)\n",
    "    \n",
    "    # Model\n",
    "    model_cv = MultinomialNB(alpha=0.5)\n",
    "    \n",
    "    # 5-fold stratified CV\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    \n",
    "    # CV scores\n",
    "    cv_scores = cross_val_score(model_cv, X_tfidf, y, cv=cv, scoring='accuracy')\n",
    "    cv_auc_scores = cross_val_score(model_cv, X_tfidf, y, cv=cv, scoring='roc_auc')\n",
    "    \n",
    "    print(f\"CV Accuracy scores: {cv_scores}\")\n",
    "    print(f\"CV Accuracy: {cv_scores.mean():.3f} ± {cv_scores.std():.3f}\")\n",
    "    print(f\"CV AUC scores: {cv_auc_scores}\")\n",
    "    print(f\"CV AUC: {cv_auc_scores.mean():.3f} ± {cv_auc_scores.std():.3f}\")\n",
    "    \n",
    "    return accuracy, auc, cm, cv_scores.mean(), cv_auc_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cc97556a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0d4e7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Testiranje: Slova preprocessing\n",
      "==================================================\n",
      "TRAIN/TEST SPLIT RESULTS:\n",
      "------------------------------\n",
      "Train uzoraka: 2064\n",
      "Test uzoraka: 516\n",
      "Accuracy:  0.779 (77.9%)\n",
      "AUC: 0.858\n",
      "Confusion Matrix:\n",
      "[[192  66]\n",
      " [ 48 210]]\n",
      "\n",
      "Misclassified: 114 total\n",
      "False Positives: 66\n",
      "False Negatives: 48\n",
      "CSV files saved: slova_preprocessing_misclassified.csv, slova_preprocessing_false_positives.csv, slova_preprocessing_false_negatives.csv\n",
      "\n",
      "==================================================\n",
      "CROSS-VALIDATION RESULTS:\n",
      "------------------------------\n",
      "CV Accuracy scores: [0.81782946 0.82170543 0.7751938  0.77131783 0.80232558 0.79844961\n",
      " 0.79844961 0.75581395 0.77131783 0.81007752]\n",
      "CV Accuracy: 0.792 ± 0.021\n",
      "CV AUC scores: [0.88696593 0.91208461 0.87320474 0.87050057 0.88774713 0.87825251\n",
      " 0.88864852 0.84937804 0.8453218  0.87723094]\n",
      "CV AUC: 0.877 ± 0.018\n"
     ]
    }
   ],
   "source": [
    "results['slova']=test_naive_bayes_combined(balanced_slova,\"Slova preprocessing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "782d4c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Testiranje: Slova_neg preprocessing\n",
      "==================================================\n",
      "TRAIN/TEST SPLIT RESULTS:\n",
      "------------------------------\n",
      "Train uzoraka: 2064\n",
      "Test uzoraka: 516\n",
      "Accuracy:  0.773 (77.3%)\n",
      "AUC: 0.860\n",
      "Confusion Matrix:\n",
      "[[191  67]\n",
      " [ 50 208]]\n",
      "\n",
      "Misclassified: 117 total\n",
      "False Positives: 67\n",
      "False Negatives: 50\n",
      "CSV files saved: slova_neg_preprocessing_misclassified.csv, slova_neg_preprocessing_false_positives.csv, slova_neg_preprocessing_false_negatives.csv\n",
      "\n",
      "==================================================\n",
      "CROSS-VALIDATION RESULTS:\n",
      "------------------------------\n",
      "CV Accuracy scores: [0.82170543 0.84108527 0.78682171 0.79069767 0.79844961 0.80620155\n",
      " 0.78294574 0.76744186 0.7751938  0.82170543]\n",
      "CV Accuracy: 0.799 ± 0.022\n",
      "CV AUC scores: [0.89009074 0.91496905 0.87638964 0.88095667 0.892855   0.88149751\n",
      " 0.89027102 0.85460609 0.84862689 0.88438195]\n",
      "CV AUC: 0.881 ± 0.018\n"
     ]
    }
   ],
   "source": [
    "results['slova_neg']=test_naive_bayes_combined(balanced_slova_neg,\"Slova_neg preprocessing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b5446129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Testiranje: POS preprocessing\n",
      "==================================================\n",
      "TRAIN/TEST SPLIT RESULTS:\n",
      "------------------------------\n",
      "Train uzoraka: 2064\n",
      "Test uzoraka: 516\n",
      "Accuracy:  0.773 (77.3%)\n",
      "AUC: 0.859\n",
      "Confusion Matrix:\n",
      "[[184  74]\n",
      " [ 43 215]]\n",
      "\n",
      "Misclassified: 117 total\n",
      "False Positives: 74\n",
      "False Negatives: 43\n",
      "CSV files saved: pos_preprocessing_misclassified.csv, pos_preprocessing_false_positives.csv, pos_preprocessing_false_negatives.csv\n",
      "\n",
      "==================================================\n",
      "CROSS-VALIDATION RESULTS:\n",
      "------------------------------\n",
      "CV Accuracy scores: [0.80620155 0.82945736 0.7751938  0.77906977 0.78682171 0.78294574\n",
      " 0.81007752 0.75193798 0.79069767 0.77906977]\n",
      "CV Accuracy: 0.789 ± 0.020\n",
      "CV AUC scores: [0.87807223 0.91247521 0.85637882 0.86875789 0.8791539  0.87554834\n",
      " 0.89537888 0.84985878 0.85157142 0.86401058]\n",
      "CV AUC: 0.873 ± 0.019\n"
     ]
    }
   ],
   "source": [
    "results['pos']=test_naive_bayes_combined(balanced_pos,\"POS preprocessing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8996d381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Testiranje: POS preprocessing NEG\n",
      "==================================================\n",
      "TRAIN/TEST SPLIT RESULTS:\n",
      "------------------------------\n",
      "Train uzoraka: 2064\n",
      "Test uzoraka: 516\n",
      "Accuracy:  0.779 (77.9%)\n",
      "AUC: 0.863\n",
      "Confusion Matrix:\n",
      "[[185  73]\n",
      " [ 41 217]]\n",
      "\n",
      "Misclassified: 114 total\n",
      "False Positives: 73\n",
      "False Negatives: 41\n",
      "CSV files saved: pos_preprocessing_neg_misclassified.csv, pos_preprocessing_neg_false_positives.csv, pos_preprocessing_neg_false_negatives.csv\n",
      "\n",
      "==================================================\n",
      "CROSS-VALIDATION RESULTS:\n",
      "------------------------------\n",
      "CV Accuracy scores: [0.80620155 0.82945736 0.7751938  0.78682171 0.79844961 0.78682171\n",
      " 0.80232558 0.75581395 0.79844961 0.79844961]\n",
      "CV Accuracy: 0.794 ± 0.019\n",
      "CV AUC scores: [0.87807223 0.9170122  0.86413076 0.87945436 0.88275945 0.88143741\n",
      " 0.8944775  0.85472628 0.85316387 0.87326483]\n",
      "CV AUC: 0.878 ± 0.018\n"
     ]
    }
   ],
   "source": [
    "results['POSNEG']=test_naive_bayes_combined(balanced_data_pos_neg,\"POS preprocessing NEG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6e3cca5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2548\n"
     ]
    }
   ],
   "source": [
    "binary_data_6=binary_data_slova[binary_data_slova['processed'].str.len()>6].copy()\n",
    "balanced_6=balansiraj_dataset(binary_data_6)\n",
    "print(len(balanced_6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f2069971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Testiranje: 6slova\n",
      "==================================================\n",
      "TRAIN/TEST SPLIT RESULTS:\n",
      "------------------------------\n",
      "Train uzoraka: 2038\n",
      "Test uzoraka: 510\n",
      "Accuracy:  0.804 (80.4%)\n",
      "AUC: 0.887\n",
      "Confusion Matrix:\n",
      "[[196  59]\n",
      " [ 41 214]]\n",
      "\n",
      "Misclassified: 100 total\n",
      "False Positives: 59\n",
      "False Negatives: 41\n",
      "CSV files saved: 6slova_misclassified.csv, 6slova_false_positives.csv, 6slova_false_negatives.csv\n",
      "\n",
      "==================================================\n",
      "CROSS-VALIDATION RESULTS:\n",
      "------------------------------\n",
      "CV Accuracy scores: [0.81960784 0.79607843 0.83921569 0.80392157 0.81960784 0.76862745\n",
      " 0.79215686 0.76078431 0.83070866 0.78346457]\n",
      "CV Accuracy: 0.801 ± 0.025\n",
      "CV AUC scores: [0.90840305 0.88152067 0.89862205 0.87512303 0.9015748  0.87718381\n",
      " 0.88447343 0.84387303 0.88610577 0.87358175]\n",
      "CV AUC: 0.883 ± 0.017\n"
     ]
    }
   ],
   "source": [
    "results['slova6']=test_naive_bayes_combined(balanced_6,\"6slova\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c15bfabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_data_60=binary_data_slova_neg[binary_data_slova_neg['processed'].str.len()>6].copy()\n",
    "balanced_60=balansiraj_dataset(binary_data_60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "21ef5b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Testiranje: 6slova_neg\n",
      "==================================================\n",
      "TRAIN/TEST SPLIT RESULTS:\n",
      "------------------------------\n",
      "Train uzoraka: 2044\n",
      "Test uzoraka: 512\n",
      "Accuracy:  0.820 (82.0%)\n",
      "AUC: 0.895\n",
      "Confusion Matrix:\n",
      "[[212  44]\n",
      " [ 48 208]]\n",
      "\n",
      "Misclassified: 92 total\n",
      "False Positives: 44\n",
      "False Negatives: 48\n",
      "CSV files saved: 6slova_neg_misclassified.csv, 6slova_neg_false_positives.csv, 6slova_neg_false_negatives.csv\n",
      "\n",
      "==================================================\n",
      "CROSS-VALIDATION RESULTS:\n",
      "------------------------------\n",
      "CV Accuracy scores: [0.76953125 0.8359375  0.8359375  0.81640625 0.77734375 0.78515625\n",
      " 0.78823529 0.82352941 0.81176471 0.81176471]\n",
      "CV Accuracy: 0.806 ± 0.023\n",
      "CV AUC scores: [0.85540771 0.909729   0.9085083  0.89007568 0.89611816 0.85217285\n",
      " 0.87832185 0.900406   0.90877215 0.88127461]\n",
      "CV AUC: 0.888 ± 0.020\n"
     ]
    }
   ],
   "source": [
    "results['slova_neg6']=test_naive_bayes_combined(balanced_60,\"6slova_neg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d95cfade",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_data_20=binary_data_pos_neg[binary_data_pos_neg['processed'].str.len()>6].copy()\n",
    "balanced_20=balansiraj_dataset(binary_data_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "927cbe59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Testiranje: 6pos_neg\n",
      "==================================================\n",
      "TRAIN/TEST SPLIT RESULTS:\n",
      "------------------------------\n",
      "Train uzoraka: 2036\n",
      "Test uzoraka: 510\n",
      "Accuracy:  0.788 (78.8%)\n",
      "AUC: 0.879\n",
      "Confusion Matrix:\n",
      "[[195  60]\n",
      " [ 48 207]]\n",
      "\n",
      "Misclassified: 108 total\n",
      "False Positives: 60\n",
      "False Negatives: 48\n",
      "CSV files saved: 6pos_neg_misclassified.csv, 6pos_neg_false_positives.csv, 6pos_neg_false_negatives.csv\n",
      "\n",
      "==================================================\n",
      "CROSS-VALIDATION RESULTS:\n",
      "------------------------------\n",
      "CV Accuracy scores: [0.79607843 0.80784314 0.80784314 0.79607843 0.81176471 0.81568627\n",
      " 0.74015748 0.75590551 0.78740157 0.7992126 ]\n",
      "CV Accuracy: 0.792 ± 0.024\n",
      "CV AUC scores: [0.87629183 0.88730315 0.88521161 0.88225886 0.90071358 0.86411171\n",
      " 0.87730175 0.84425569 0.86124372 0.88746977]\n",
      "CV AUC: 0.877 ± 0.015\n"
     ]
    }
   ],
   "source": [
    "results['6pos_neg']=test_naive_bayes_combined(balanced_20,\"6pos_neg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e718f439",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_data_20=binary_data_pos[binary_data_pos['processed'].str.len()>6].copy()\n",
    "balanced_20=balansiraj_dataset(binary_data_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ee840aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Testiranje: 6pos\n",
      "==================================================\n",
      "TRAIN/TEST SPLIT RESULTS:\n",
      "------------------------------\n",
      "Train uzoraka: 2028\n",
      "Test uzoraka: 508\n",
      "Accuracy:  0.756 (75.6%)\n",
      "AUC: 0.855\n",
      "Confusion Matrix:\n",
      "[[183  71]\n",
      " [ 53 201]]\n",
      "\n",
      "Misclassified: 124 total\n",
      "False Positives: 71\n",
      "False Negatives: 53\n",
      "CSV files saved: 6pos_misclassified.csv, 6pos_false_positives.csv, 6pos_false_negatives.csv\n",
      "\n",
      "==================================================\n",
      "CROSS-VALIDATION RESULTS:\n",
      "------------------------------\n",
      "CV Accuracy scores: [0.7519685  0.79527559 0.82677165 0.77559055 0.78740157 0.78346457\n",
      " 0.74703557 0.7312253  0.80237154 0.7944664 ]\n",
      "CV Accuracy: 0.780 ± 0.027\n",
      "CV AUC scores: [0.8512617  0.88238576 0.89673879 0.8520057  0.85963172 0.87866576\n",
      " 0.86389201 0.84376953 0.87139108 0.88670166]\n",
      "CV AUC: 0.869 ± 0.016\n"
     ]
    }
   ],
   "source": [
    "results['6pos']=test_naive_bayes_combined(balanced_20,\"6pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d97d7256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Testiranje: original\n",
      "==================================================\n",
      "TRAIN/TEST SPLIT RESULTS:\n",
      "------------------------------\n",
      "Train uzoraka: 2064\n",
      "Test uzoraka: 516\n",
      "Accuracy:  0.742 (74.2%)\n",
      "AUC: 0.817\n",
      "Confusion Matrix:\n",
      "[[193  65]\n",
      " [ 68 190]]\n",
      "\n",
      "Misclassified: 133 total\n",
      "False Positives: 65\n",
      "False Negatives: 68\n",
      "CSV files saved: original_misclassified.csv, original_false_positives.csv, original_false_negatives.csv\n",
      "\n",
      "==================================================\n",
      "CROSS-VALIDATION RESULTS:\n",
      "------------------------------\n",
      "CV Accuracy scores: [0.80620155 0.77906977 0.79457364 0.80620155 0.71705426 0.76744186\n",
      " 0.77906977 0.72868217 0.69379845 0.78294574]\n",
      "CV Accuracy: 0.766 ± 0.037\n",
      "CV AUC scores: [0.88143741 0.86989965 0.87470705 0.86124632 0.83144042 0.85187188\n",
      " 0.87230335 0.83423472 0.81052821 0.86070549]\n",
      "CV AUC: 0.855 ± 0.022\n"
     ]
    }
   ],
   "source": [
    "binary_data_balanced=balansiraj_dataset(binary_data)\n",
    "binary_data_balanced['processed']=binary_data_balanced['text']\n",
    "results['original']=test_naive_bayes_combined(binary_data_balanced,\"original\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d8516b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8860f44",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
